一、语言模型
1️⃣自回归 (AR) = 猜下一个词——文本生成任务的首选模型，因此比掩码语言模型更受欢迎。
2️⃣掩码 (MLM) = 猜中间的词——常用于情感分析和文本分类等非生成任务
目前最前沿的研究（如 Google 的 T5 或 GLM）尝试把两者结合起来，即：用“填空”的方式去训练，但让模型具备“续写”的能力。
虽然“填空”的模型（如 BERT）在理解复杂句子上很强，但现在的大模型（LLM）主流几乎全是“背诵”架构（自回归）

3️⃣生成式模型 (Generative Model)是一个广义的概念，只要一个模型的目标是学习数据的概率分布，并能生成新的数据，它就叫生成式模型。
它不仅限于文本，还包括图像（如 Midjourney 使用的扩散模型）、音频等。
4️⃣RNN/LSTM/Seq2Seq
5️⃣BERT/Transformer Encoder
6️⃣多数据模态融合-理解图像视频蛋白质结构等-更强大-
传统：一直按照数据模态划分：自然语言处理只处理文本，计算机视觉只处理视觉。
纯文本模型可用于翻译和垃圾邮件检测等任务，纯图像模型可用于目标检测和图像分类，
纯音频模型则可用于语音识别（语音转文本，speech-to-text，STT）和语音合成（文本转语音，text-to-speech，TTS）​。
7️⃣多模态模型-生成式多模态大模型
语言模型可以通过自监督的方式进行训练，而许多其他模型则需要监督训练。
1️⃣自监督训练-不需要人工标注-从输入数据中自行推断标签-语言建模就是一种自监督学习-创建更大数据集规模
语言模型可以直接从文本序列中学习，而不需要任何标注。由于文本序列随处可见——书籍、博客文章、新闻报道以及Reddit评论等，因此可以构建大量的训练数据，使语言模型能够扩展为LLM。
2️⃣监督训练-标注数据-耗时成本高

3️⃣无监督学习-不需要标签。

大数据集搭配大模型训练，大模型拥有更强的学习能力-反之小数据集搭配小模型训练；
把小数据集喂给大模型-浪费计算资源、过拟合;
那把大数据集喂给小模型-欠拟合；
